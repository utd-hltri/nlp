{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_language_modeling_glove.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
        "cell_type": "markdown",
        "metadata": {
          "id": "view-in-github",
          "colab_type": "text"
        },
        "source": [
          "<a href=\"https://colab.research.google.com/github/utd-hltri/nlp/blob/main/hw1/neural_language_modeling_glove.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
        ]
    },
    {
      "metadata": {
        "id": "ItXfxkxvosLH"
      },
      "cell_type": "markdown",
      "source": [
        "# Language Modeling with newswire text"
      ]
    },
    {
      "metadata": {
        "id": "Eg62Pmz3o83v"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "This notebook constructs, trains, and evaluates a simple feed-forward neural language model.\n",
        "\n",
        "We'll use the [Reuters newswire dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/reuters) that contains the text of 11,228 newswires from Reuters. These are split into 8,982 newswires for training and 2246 newswires for testing.\n",
        "\n",
        "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/)."
      ]
    },
    {
      "metadata": {
        "id": "2ew7HTbPpCJH"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "N = 5\n",
        "vocab_size=10000\n",
        "\n",
        "print('Creating {}-gram LM with vocab size={}'.format(N+1, vocab_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iAsKG535pHep"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the Reuters dataset\n",
        "\n",
        "The Reuters dataset comes packaged with TensorFlow. It has already been preprocessed such that the newswires (sequences of words) have been converted to sequences of integers, where each integer represents a specific word in a dictionary.\n",
        "\n",
        "The following code downloads the Reuters dataset to your machine (or uses a cached copy if you've already downloaded it):"
      ]
    },
    {
      "metadata": {
        "id": "zXXx5Oc3pOmN"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import reuters\n",
        "\n",
        "(train_data, _), (test_data, _) = reuters.load_data(num_words=vocab_size, seed=1337, test_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "odr-KlzO-lkL"
      },
      "cell_type": "markdown",
      "source": [
        "The argument `num_words=vocab_size` keeps the top `vocab_size=10,000` most frequently occurring words in the training data. The rare words are discarded to keep the size of the data manageable. Increasing this limit will result in a larger model that, while more accurate, will take longer to train and could result in overfitting.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "l50X3GfjpU4r"
      },
      "cell_type": "markdown",
      "source": [
        "## Explore the data \n",
        "\n",
        "Let's take a moment to understand the format of the data. The dataset comes preprocessed: each example is an array of integers representing the words of the newswire."
      ]
    },
    {
      "metadata": {
        "id": "y8qCnve_-lkO"
      },
      "cell_type": "code",
      "source": [
        "print(\"Training entries: {}, Testing entries: {}\".format(len(train_data), len(test_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RnKvHWW4-lkW"
      },
      "cell_type": "markdown",
      "source": [
        "The text of reviews have been converted to integers, where each integer represents a specific word in a dictionary. Here's what the first review looks like:"
      ]
    },
    {
      "metadata": {
        "id": "QtTS4kpEpjbi"
      },
      "cell_type": "code",
      "source": [
        "print(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hIE4l_72x7DP"
      },
      "cell_type": "markdown",
      "source": [
        "Newswires may be different lengths. The below code shows the number of words in the first and second newswires. Since inputs to a neural network must be the same length, we'll need to resolve this later."
      ]
    },
    {
      "metadata": {
        "id": "X-6Ii9Pfx6Nr"
      },
      "cell_type": "code",
      "source": [
        "len(train_data[0]), len(train_data[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4wJg2FiYpuoX"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert the integers back to words\n",
        "\n",
        "It may be useful to know how to convert integers back to text. Here, we'll create a helper function to query a dictionary object that contains the integer to string mapping:"
      ]
    },
    {
      "metadata": {
        "id": "tr5s_1alpzop"
      },
      "cell_type": "code",
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "word_index = reuters.get_word_index()\n",
        "\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()} \n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_newswire(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U3CNRvEZVppl"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can use the `decode_review` function to display the text for the first review:"
      ]
    },
    {
      "metadata": {
        "id": "s_OqxmH6-lkn"
      },
      "cell_type": "code",
      "source": [
        "decode_newswire(train_data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFP_XKVRp4_S"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare the data\n",
        "\n",
        "The newswires—the variable-length arrays of integers—must be converted to n-gram tensors before being fed into the neural network.\n",
        "\n",
        "The job of a language model is to predict the next word in a sequence given the previous N words. There fore, the training data for our neural language model will be `(N-gram, label)` tuples where the *label* is the word following the N-gram in some newswire in our corpus.\n",
        "\n",
        "To create this training data, we extract each N-gram from each newswire and save the following word as the label for that N-gram."
      ]
    },
    {
      "metadata": {
        "id": "2jQv-omsHurp"
      },
      "cell_type": "code",
      "source": [
        "def convert_sequences_to_ngrams_with_labels(sequences):\n",
        "  ngrams = []\n",
        "  labels =[]\n",
        "  for seq in sequences:\n",
        "    idx = 0\n",
        "    while idx+N < len(seq)-1:\n",
        "      ngrams.append(seq[idx:idx+N])\n",
        "      labels.append(seq[idx+N])\n",
        "      idx += 1\n",
        "  return np.asarray(ngrams), np.asarray(labels)\n",
        "\n",
        "train_data, train_labels = convert_sequences_to_ngrams_with_labels(train_data)\n",
        "test_data, test_labels = convert_sequences_to_ngrams_with_labels(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VO5MBpyQdipD"
      },
      "cell_type": "markdown",
      "source": [
        "Let's look at some of the examples now:"
      ]
    },
    {
      "metadata": {
        "id": "USSSBnkE-lky"
      },
      "cell_type": "code",
      "source": [
        "print(\"{} -> {}\".format(decode_newswire(train_data[0]), reverse_word_index[train_labels[0]]))\n",
        "print(\"{} -> {}\".format(decode_newswire(train_data[1]), reverse_word_index[train_labels[1]]))\n",
        "print(\"{} -> {}\".format(decode_newswire(train_data[2]), reverse_word_index[train_labels[2]]))\n",
        "print(\"{} -> {}\".format(decode_newswire(train_data[3]), reverse_word_index[train_labels[3]]))\n",
        "print(\"{} -> {}\".format(decode_newswire(train_data[4]), reverse_word_index[train_labels[4]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "43xaHjbkXf_f"
      },
      "cell_type": "markdown",
      "source": [
        "### Download the Word Embeddings\n"
      ]
    },
    {
      "metadata": {
        "id": "Mre7hkjwYgGb"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isfile('glove.6B.50d.txt'):\n",
        "  !wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "  !unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nc3E-BAfdsaS"
      },
      "cell_type": "markdown",
      "source": [
        "Unpack the word emebddings into a dict of word -> embedding vector."
      ]
    },
    {
      "metadata": {
        "id": "tGfjN2c8YpbH"
      },
      "cell_type": "code",
      "source": [
        "word2vec = {}\n",
        "with open('glove.6B.50d.txt') as f:\n",
        "  for line in f:\n",
        "    fields = line.split()\n",
        "    word2vec[fields[0]] = np.asarray(fields[1:])\n",
        "print('Loaded %d GloVe embeddings' % len(word2vec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "--08sR1kd1Fp"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, create the embedding matrix using only those embeddings which represent one of the 10,000 words in our dataset."
      ]
    },
    {
      "metadata": {
        "id": "_4AHhsAsavyH"
      },
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 50), dtype=float)\n",
        "for word, i in word_index.items():\n",
        "  if i < vocab_size:\n",
        "    if word in word2vec:\n",
        "      embedding_matrix[i] = word2vec[word]\n",
        "    else:\n",
        "      embedding_matrix[i] = word_index['<UNK>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LLC02j2g-llC"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "The neural network is created by stacking layers—this requires two main architectural decisions:\n",
        "\n",
        "* How many layers to use in the model?\n",
        "* How many *hidden units* to use for each layer?\n",
        "\n",
        "In this example, the input data consists of an array of word-indices. The labels to predict are single word-indices. Let's build a model for this problem:"
      ]
    },
    {
      "metadata": {
        "id": "xpKOoWgu-llD"
      },
      "cell_type": "code",
      "source": [
        "# input shape is the vocabulary count used for the newswire (10,000 words)\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, 50, weights=[embedding_matrix]))\n",
        "model.add(keras.layers.Reshape([50*N]))\n",
        "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(vocab_size, activation=tf.nn.softmax))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PbKQ6mucuKL"
      },
      "cell_type": "markdown",
      "source": [
        "The layers are stacked sequentially to build the classifier:\n",
        "\n",
        "1. The first layer is an `Embedding` layer. This layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: `(batch, N, embedding)`.\n",
        "2. Next, a `Reshape` layer flattens each (N, 50) dimension N-gram matrix into a (N\\*50) dimension vector. This allows the model to process the entire N-gram in the same fully-connected layer, preserving sequential information.\n",
        "3. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n",
        "4. The last layer is densely connected with a 10000 output nodes -- one for each word in the dictionary. Using the `softmax` activation function, this produces a probability distribution over the *next* word following the input N-gram."
      ]
    },
    {
      "metadata": {
        "id": "0XMwnDOp-llH"
      },
      "cell_type": "markdown",
      "source": [
        "### Hidden units\n",
        "\n",
        "The above model has two intermediate or \"hidden\" layers, between the input and output. The number of outputs (units, nodes, or neurons) is the dimension of the representational space for the layer. In other words, the amount of freedom the network is allowed when learning an internal representation.\n",
        "\n",
        "If a model has more hidden units (a higher-dimensional representation space), and/or more layers, then the network can learn more complex representations. However, it makes the network more computationally expensive and may lead to learning unwanted patterns—patterns that improve performance on training data but not on the test data. This is called *overfitting*."
      ]
    },
    {
      "metadata": {
        "id": "L4EqVWg4-llM"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss function and optimizer\n",
        "\n",
        "A model needs a loss function and an optimizer for training. Since this is a categorical classification problem and the model outputs a probability distribution(a vector of values in [0,1] that sum to 1), we'll use the categorical cross-entropy loss function. Specifically, we use the `sparse_categorical_cross_entropy` loss function to avoid instantiating each 10,000-dimension one-hot label vector.\n",
        "\n",
        "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, cross-entropy is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
        "\n",
        "We can evaluate our model using [Perplexity](https://en.wikipedia.org/wiki/Perplexity) and top-5 accuracy. Perplexity measures how likely our model thought the actual next word was. Lower values are better.\n",
        "Top-5 accuracy is the percentage of N-grams for which the next word is among the top-5 most likely according to the model.\n",
        "\n",
        "Now, configure the model to use an optimizer and a loss function:"
      ]
    },
    {
      "metadata": {
        "id": "Mr0GP-cQ-llN"
      },
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "\n",
        "def perplexity(y_true, y_pred):\n",
        "  cross_entropy = K.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "  perplexity = K.pow(2.0, cross_entropy)\n",
        "  return perplexity\n",
        "  \n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=[perplexity, 'sparse_top_k_categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hCWYwkug-llQ"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a validation set\n",
        "\n",
        "When training, we want to check the quality of the model on data it hasn't seen before. Create a *validation set* by setting apart 10,000 examples from the original training data. (Why not use the testing set now? Our goal is to develop and tune our model using only the training data, then use the test data just once to evaluate our model)."
      ]
    },
    {
      "metadata": {
        "id": "-NpcXY9--llS"
      },
      "cell_type": "code",
      "source": [
        "x_val = train_data[:10000]\n",
        "partial_x_train = train_data[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "35jv_fzP-llU"
      },
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Train the model for 40 epochs in mini-batches of 512 samples. This is 40 iterations over all samples in the `x_train` and `y_train` tensors. While training, monitor the model's loss and perplexity on the 10,000 samples from the validation set:"
      ]
    },
    {
      "metadata": {
        "id": "tXSGrjWZ-llW"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=40,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val),\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9EEGuDVuzb5r"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model\n",
        "\n",
        "And let's see how the model performs. Three values will be returned. Loss (a number which represents our error, lower values are better), accuracy@5, and preplexity."
      ]
    },
    {
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "\n",
        "print('\\n'.join('%s: %.4f'%t for t in zip(['loss', 'perplexity', 'acc@5'], results)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z1iEXVTR0Z2t"
      },
      "cell_type": "markdown",
      "source": [
        "This fairly naive approach achieves an accuracy@5 of about 39%. With more advanced approaches, the model should get much higher.\n"
      ]
    },
    {
      "metadata": {
        "id": "5KggXVeL-llZ"
      },
      "cell_type": "markdown",
      "source": [
        "## Create a graph of accuracy and loss over time\n",
        "\n",
        "`model.fit()` returns a `History` object that contains a dictionary with everything that happened during training:"
      ]
    },
    {
      "metadata": {
        "id": "VcvSXvhp-llb"
      },
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRKsqL40-lle"
      },
      "cell_type": "markdown",
      "source": [
        "There are six entries: one for each monitored metric during training and validation. We can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy:"
      ]
    },
    {
      "metadata": {
        "id": "nGoYf2Js-lle"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "perp = history_dict['perplexity']\n",
        "val_perp = history_dict['val_perplexity']\n",
        "top5acc = history_dict['sparse_top_k_categorical_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "val_top5acc = history_dict['val_sparse_top_k_categorical_accuracy']\n",
        "\n",
        "epochs = range(1, len(perp) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6hXx-xOv-llh"
      },
      "cell_type": "code",
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, perp, 'bo', label='Training perplexity')\n",
        "plt.plot(epochs, val_perp, 'b', label='Validation perplexity')\n",
        "plt.title('Training and validation perpelexity')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tZ22jUcRSTtp"
      },
      "cell_type": "code",
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "plt.plot(epochs, top5acc, 'bo', label='Training top5acc')\n",
        "plt.plot(epochs, val_top5acc, 'b', label='Validation top5acc')\n",
        "plt.title('Training and validation top5acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('top5acc')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oFEmZ5zq-llk"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "In these plots, the dots represent the training loss and metrics, and the solid lines are the validation loss and metrics.\n",
        "\n",
        "Notice the training loss *decreases* with each epoch and the training accuracy *increases* with each epoch. This is expected when using a gradient descent optimization—it should minimize the desired quantity on every iteration.\n",
        "\n",
        "This isn't the case for the validation loss and accuracy—they seem to peak after about twenty and seven epochs, respectively. This is an example of overfitting: the model performs better on the training data than it does on data it has never seen before. After this point, the model over-optimizes and learns representations *specific* to the training data that do not *generalize* to test data.\n",
        "\n",
        "For this particular case, we could prevent overfitting by simply stopping the training after twenty or so epochs.\n",
        "\n",
        "Interestingly, while the loss does decrease initially for the validation set, the perplexity continuously increases. Can you think of a reason why this might be?"
      ]
    }
  ]
}