{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_summarization_pegasus.ipynb",
      "version": "0.3.2",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
        "cell_type": "markdown",
        "metadata": {
          "id": "view-in-github",
          "colab_type": "text"
        },
        "source": [
          "<a href=\"https://colab.research.google.com/github/utd-hltri/nlp/blob/main/hw2/neural_summarization_pegasus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
        ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx1OQd2_WHok"
      },
      "outputs": [],
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2022 Maxwell Weinzierl\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Summarization with PEGASUS\n",
        "\n",
        "This notebook utilizes PEGASUS, a State-Of-The-Art (SOTA) neural transformer model pre-trained on extracted gap-sentences for\n",
        "abstractive summarization. \n",
        "The paper which introduces PEGASUS can be found here:\n",
        "https://arxiv.org/pdf/1912.08777.pdf"
      ],
      "metadata": {
        "id": "IpFmigG0XWC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages and Libraries\n",
        "We will utilize the deep learning library PyTorch this time as opposed to TensorFlow. PyTorch (https://pytorch.org/) has become the most popular deep learning library for research to-date: http://horace.io/pytorch-vs-tensorflow/\n",
        "\n",
        "![](https://www.assemblyai.com/blog/content/images/2021/12/Fraction-of-Papers-Using-PyTorch-vs.-TensorFlow.png)"
      ],
      "metadata": {
        "id": "WouzhYvvYBFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace Transformers\n",
        "\n",
        "Next we will install the `transformers` library, built by HuggingFace. This library makes it extremely easy to use SOTA neural NLP models with PyTorch. See the HuggingFace website to browse all the publically available models: https://huggingface.co/models\n",
        "\n",
        "## HuggingFace Datasets\n",
        "HuggingFace also provides a library called `datasets` for downloading and utilizing common NLP datasets: https://huggingface.co/datasets\n",
        "\n",
        "## SentencePiece Tokenizer\n",
        "The SentencePiece tokenizer library is required for the PEGASUS model\n",
        "\n",
        "## Model Summary\n",
        "TorchInfo is a nice little library to provide a summary of model sizes and layers. We install it below to visualize the size of our models."
      ],
      "metadata": {
        "id": "EMz34AztC1lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets sentencepiece torchinfo"
      ],
      "metadata": {
        "id": "1YvJqRvi4tHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "from torchinfo import summary\n",
        "from textwrap import wrap"
      ],
      "metadata": {
        "id": "9jx9xw1y4hIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print('CUDA Enabled: ', torch.cuda.is_available())\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  print(f'  {device} - ' + torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print(f'  {device}')"
      ],
      "metadata": {
        "id": "RZ3Pa-paWI2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above cell should include a torch library with \"+cu...\" to denote PyTorch is installed with CUDA capabilities. CUDA should be enabled with at least one device. Typically a Tesla K80 is the GPU I get on Google Colab, but others may be assigned as resources are made available. If you are unable to reserve a GPU instance then the device will be \"cpu\" and the code will run much slower, but still work."
      ],
      "metadata": {
        "id": "ZJDQtKsTZhOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Summarization Models\n",
        "\n",
        "Below we load our neural summarization model. We load the model and the tokenizer from the `model_name` from HuggingFace. The library will automatically download all required model weights, config files, and tokenizers.\n",
        "\n",
        "We then move the model to the `cuda:0` device (our GPU) and turn on eval mode to avoid dropout randomness.\n",
        "\n",
        "Finally, we print a summary of our model."
      ],
      "metadata": {
        "id": "OI-_5wOvcXm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "\n",
        "model_name = 'google/pegasus-xsum' #@param [\"google/pegasus-xsum\"]\n",
        "\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "# move model to GPU device\n",
        "model.to(device)\n",
        "# turn on EVAL mode so drop-out layers do not randomize outputs\n",
        "model.eval()\n",
        "# create model summary\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "SZsV1DWOchJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization Datasets\n",
        "We will examine the Extreme Summarization (XSum) Dataset. https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset\n",
        "\n",
        "Feel free to play around with the other Summarization datasets, or find your own on HuggingFace Datasets: https://huggingface.co/datasets?task_categories=task_categories:question-answering&sort=downloads"
      ],
      "metadata": {
        "id": "DmQTA2_7klsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "#@title Dataset\n",
        "\n",
        "dataset = 'xsum' #@param [\"xsum\", \"cnn_dailymail\"]\n",
        "data = load_dataset(dataset)\n",
        "ds = data['validation']\n",
        "data_size = len(ds)\n",
        "print(ds)"
      ],
      "metadata": {
        "id": "nTIvI2HpkmIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting the Dataset\n",
        "\n",
        "We can look at individual examples in the validation collection of SQUAD v2 to get a feeling for the types of questions and answers."
      ],
      "metadata": {
        "id": "EmA-JENXoUtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example { run: \"auto\" }\n",
        "example_index = 0 #@param {type:\"slider\", min:0, max:11331, step:1}\n",
        "example = ds[example_index]\n",
        "print('Document: ')\n",
        "for line in wrap(example['document'], 50):\n",
        "  print(f'  {line}')\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "TA0pl-Btlpen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specific Example\n",
        "We will use the below example to follow the prediction process of the model"
      ],
      "metadata": {
        "id": "Y-X7yze6rRNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_index = 2161\n",
        "example = ds[example_index]\n",
        "print('Document: ')\n",
        "for line in wrap(example['document'], 50):\n",
        "  print(f'  {line}')\n",
        "print('Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "_JwW3EvPrF5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "We will tokenize the above example using the HuggingFace tokenizer:"
      ],
      "metadata": {
        "id": "sRDfru4IkeAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will tokenize a single example question and context,\n",
        "# and we will move these tensors to the GPU device:\n",
        "inputs = tokenizer(example['document'], return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "print('Inputs to model: ')\n",
        "print(f'  {inputs.keys()}')"
      ],
      "metadata": {
        "id": "B_j4dH1tfiGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the inputs to the model will contain a few tensors, but the most\n",
        "# important tensor is the \"input_ids\":\n",
        "input_ids = inputs['input_ids'][0]\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "BhBBZvvbqIRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# these are the token ids of the input. We can convert back to text tokens like so:\n",
        "input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "for line in wrap(str(input_tokens), 50):\n",
        "  print(line)\n",
        "\n",
        "# Notice that we have added a </s> token to denote the end of the sequence,\n",
        "# and the SentencePiece tokenizer has split some words up into pieces, such as \n",
        "# '▁PG', '&', 'E' from PG&E and\n",
        "# '▁shutoff', 's' from shutoffs"
      ],
      "metadata": {
        "id": "CrrPIqMjqMke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Model\n",
        "\n",
        "Next we will run the model on the above example"
      ],
      "metadata": {
        "id": "P4GXlLhfbj7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# the outputs will contain decoded token ids\n",
        "# based on the estimated most likely summary sequence\n",
        "# using greedy decoding\n",
        "summary_ids = model.generate(\n",
        "    **inputs\n",
        ")[0]\n",
        "print(summary_ids)"
      ],
      "metadata": {
        "id": "3SH5GFb7avK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we convert these token ids back to tokens: \n",
        "summary_tokens = tokenizer.convert_ids_to_tokens(summary_ids)\n",
        "for line in wrap(str(summary_tokens), 50):\n",
        "  print(line)"
      ],
      "metadata": {
        "id": "UNBQVZAyrtQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can then transform these tokens to a normal string:\n",
        "summary = tokenizer.convert_tokens_to_string(summary_tokens)\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')\n",
        "print('Generated Summary:')\n",
        "for line in wrap(summary, 50):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "KUtPSG6Pq9p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling Summaries\n",
        "We will first define a `run_model` function to do all of the above for an example."
      ],
      "metadata": {
        "id": "kqpw8AwMr0AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run this cell when you swap models\n",
        "def run_model(example, **generate_args):\n",
        "  # we will tokenize a single example document,\n",
        "  # and we will move these tensors to the GPU device:\n",
        "  inputs = tokenizer(example['document'], return_tensors=\"pt\", truncation=True).to(device)\n",
        "  # the outputs will contain decoded token ids\n",
        "  # based on the estimated most likely summary sequence\n",
        "  # using various decoding options\n",
        "  multi_summary_ids = model.generate(\n",
        "      input_ids=inputs['input_ids'],   \n",
        "      attention_mask=inputs['attention_mask'],\n",
        "      **generate_args\n",
        "  )\n",
        "  # converts token ids back to strings for multiple summaries\n",
        "  summaries = tokenizer.batch_decode(\n",
        "      multi_summary_ids, \n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "  return summaries\n"
      ],
      "metadata": {
        "id": "07W9NtfvsCP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Strategies\n",
        "There are various ways to produce samples from a sequence generating model. \n",
        "Above we utilized Greedy search, which picks the maximum probability token at\n",
        "every opportunity. This can miss out on other tokens which may have \n",
        "a lower conditional probability, but produce a higher joint sentence probability\n",
        "after futher token generation.\n",
        "The following article summarizes many popular generating strategies: https://huggingface.co/blog/how-to-generate"
      ],
      "metadata": {
        "id": "pJE5WYbf8jCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Search\n",
        "![](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "-iof4jQt9H7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = run_model(example)[0]\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')\n",
        "  \n",
        "print('Greedy Summary:')\n",
        "for line in wrap(summary, 50):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "e0-T4G9y9UK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search\n",
        "![](https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png)\n"
      ],
      "metadata": {
        "id": "dwtfdKEo9Z0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    num_beams=10, \n",
        "    num_return_sequences=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Beam Summaries:')\n",
        "for beam, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Beam #{beam} Summary:')\n",
        "  for line in wrap(summary, 50):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "AZ42XEGb-dL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problems with Beam Search\n",
        "Beam search and all deterministic generating approaches are rarely suprising.\n",
        "This leads to an almost robotic sounding result, where only high-probability English words are selected. \n",
        "\n",
        "![](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n",
        "\n",
        "In reality, language is often suprising, with unlikely words showing up all the time! Therefore, we want to consider approaches which randomly sample from the conditional distribution produced by our model."
      ],
      "metadata": {
        "id": "G2hpG4JWAcO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "Now there are multiple approaches to random sampling from the $P(w_t|w_{1:t-1})$ conditional distribution. The first approach is just to directly sample:"
      ],
      "metadata": {
        "id": "Z-PPtxkcBG9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Sampled Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 50):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "FzPhhRPt_M7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature\n",
        "\n",
        "We can modify $P(w_t|w_{1:t-1})$ to be more or less \"suprising\", by making the distribution sharper or more flat with the `temperature` parameter. A lower temperature ($t<1.0$) leads to a sharper distribution, which will have a higher probability of sampling from high probability tokens.\n"
      ],
      "metadata": {
        "id": "EjwnDruNBgPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_k=0,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Low Temperature Sampled Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 50):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "ivyMZJDh_gyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A higher temperature ($t>1.0$) leads to a flatter distribution, which will have a higher probability of sampling from low probability tokens."
      ],
      "metadata": {
        "id": "SnLxtDFuCNju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_k=0,\n",
        "    temperature=1.3\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('High Temperature Sampled Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 50):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "ugymlabo_qBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-K Sampling\n",
        "Top-K sampling restricts $P(w_t|w_{1:t-1})$ to only allow sampling from the top-k probability tokens. In effect, this rebalances $P(w_t|w_{1:t-1})$ to remove all probability mass from non top-k tokens to be redistributed to top-k tokens, such that only top-k tokens get sampled. This approach avoids sampling extremely low probability tokens, and thus potentially ruining the sequence."
      ],
      "metadata": {
        "id": "Z30xo7WLCRAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_k=50,\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Top-K Sampling Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 50):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "7CS0RpVG_vP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-P Sampling\n",
        "Top-P sampling restricts $P(w_t|w_{1:t-1})$ to only allow sampling from the tokens which have a sum total probability mass greater than p. In other words, the probabilities $P(w_t|w_{1:t-1})$ are sorted, from largest to smallest, and only tokens from the first top-p probability mass are available to be sampled from. The probability mass is then redistributed among these top-p tokens."
      ],
      "metadata": {
        "id": "sIEbvfI8CyMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_p=0.90, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Top-P Sampling Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 50):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "95Kg-EKi_6gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-P and Top-K Sampling\n",
        "We can also perform both Top-P and Top-K sampling together, which provides multiple constraints on which tokens we can sample from $P(w_t|w_{1:t-1})$."
      ],
      "metadata": {
        "id": "gOHs5QFBDsD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_p=0.90, \n",
        "    top_k=50, \n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Top-P AND Top-K Sampling Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 50):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "tgGREitAAKyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examine Summaries\n",
        "Now we will perform the above process for a few examples. We will first define a `generate` function to do all of the above for an example."
      ],
      "metadata": {
        "id": "QumL1aaZswfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(example, strategy):\n",
        "  if strategy == 'greedy':\n",
        "    summary = run_model(example)[0]\n",
        "  elif strategy == 'beam':\n",
        "    summary = run_model(\n",
        "        example,\n",
        "        num_beams=10, \n",
        "        num_return_sequences=1, \n",
        "        early_stopping=True\n",
        "    )[0]\n",
        "  elif strategy == 'sample':\n",
        "    summary = run_model(\n",
        "        example,\n",
        "        do_sample=True, \n",
        "        num_return_sequences=1,\n",
        "        top_k=0, \n",
        "    )[0]\n",
        "  elif strategy == 'top-k':\n",
        "    summary = run_model(\n",
        "        example,\n",
        "        do_sample=True, \n",
        "        num_return_sequences=1,\n",
        "        top_k=50, \n",
        "    )[0]\n",
        "  elif strategy == 'top-p':\n",
        "    summary = run_model(\n",
        "        example,\n",
        "        do_sample=True, \n",
        "        num_return_sequences=1,\n",
        "        top_p=0.90, \n",
        "        top_k=0, \n",
        "    )[0]\n",
        "  elif strategy == 'top-p-k':\n",
        "    summary = run_model(\n",
        "        example,\n",
        "        do_sample=True, \n",
        "        num_return_sequences=1,\n",
        "        top_p=0.90, \n",
        "        top_k=50, \n",
        "    )[0]\n",
        "  else:\n",
        "    raise ValueError(f'Unknown generator strategy: {strategy}')\n",
        "  return summary"
      ],
      "metadata": {
        "id": "cU95bOoEIlyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "Change the example index and view the model's predictions below for 10 different examples. For each example, compare the results for each strategy. Manually judge whether each strategy for each of the 10 examples is correct, for a total of 60 judgements. \n",
        "\n",
        "Discuss how accurately the model summarized the documents and whether they lined up with the annotated summaries of the examples. Report the results in your report. "
      ],
      "metadata": {
        "id": "_9T_CyZuKHlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example { run: \"auto\" }\n",
        "example_index = 0 #@param {type:\"slider\", min:0, max:11331, step:1}\n",
        "strategy = 'greedy' #@param [\"greedy\", \"beam\", \"sample\", \"top-k\", \"top-p\", \"top-p-k\"]\n",
        "\n",
        "example = ds[example_index]\n",
        "print('Document: ')\n",
        "for line in wrap(example['document'], 50):\n",
        "  print(f'  {line}')\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 50):\n",
        "  print(f'  {line}')\n",
        "\n",
        "summary = generate(example, strategy)\n",
        "\n",
        "print(f'Generated Summary: ')\n",
        "for line in wrap(summary, 50):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "Ix5Olgx9q-x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report Format\n",
        "\n",
        "You should have the following in your report:\n",
        "\n",
        "| Strategy      | Accuracy |\n",
        "| ----------- | ----------- |\n",
        "| greedy      | ...       |\n",
        "| beam      | ...       |\n",
        "| sample      | ...       |\n",
        "| top-k      | ...       |\n",
        "| top-p      | ...       |\n",
        "| top-p-k      | ...       |\n",
        "\n",
        "\n",
        "Calculate the accuracy of each summary strategy by adding up the number of correct examples (by your own judgement) and dividing by 10 (the total number of examples you should evaluate). \n",
        "\n",
        "Also include an example prediction that has a judged answer and compare it to the predictions by each strategy. Try to find an example where the strategies differ."
      ],
      "metadata": {
        "id": "A0f3TGGVuhrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mGUQP0Awsdg8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}