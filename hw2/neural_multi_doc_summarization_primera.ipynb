{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_multi_doc_summarization_primera.ipynb",
      "version": "0.3.2",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
        "cell_type": "markdown",
        "metadata": {
          "id": "view-in-github",
          "colab_type": "text"
        },
        "source": [
          "<a href=\"https://colab.research.google.com/github/utd-hltri/nlp/blob/main/hw2/neural_multi_doc_summarization_primera.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
        ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx1OQd2_WHok"
      },
      "outputs": [],
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2022 Maxwell Weinzierl\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Document Neural Summarization with PRIMERA\n",
        "\n",
        "This notebook utilizes PRIMERA: Pyramid-based Masked Sentence Pre-training for\n",
        "Multi-document Summarization\n",
        "The paper which introduces PRIMERA can be found here:\n",
        "https://openreview.net/pdf?id=xBz8_ZZWM8d\n",
        "\n",
        "This notebook utilizes code from the official repo: https://github.com/allenai/PRIMER"
      ],
      "metadata": {
        "id": "IpFmigG0XWC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages and Libraries\n",
        "We will utilize the deep learning library PyTorch this time as opposed to TensorFlow. PyTorch (https://pytorch.org/) has become the most popular deep learning library for research to-date: http://horace.io/pytorch-vs-tensorflow/\n",
        "\n",
        "![](https://www.assemblyai.com/blog/content/images/2021/12/Fraction-of-Papers-Using-PyTorch-vs.-TensorFlow.png)"
      ],
      "metadata": {
        "id": "WouzhYvvYBFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LongFormer\n",
        "LongFormer is the transformer library upon which PRIMERA is built.\n",
        "See https://arxiv.org/pdf/2004.05150.pdf\n",
        "\n",
        "You may need to restart the notebook after installing these libraries."
      ],
      "metadata": {
        "id": "y5zdo60swJOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning==1.3.5 spacy==2.3.3 nltk==3.6.1 tqdm==4.49.0 datasets==1.6.2"
      ],
      "metadata": {
        "id": "UpKrHDKjwJYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/allenai/longformer.git"
      ],
      "metadata": {
        "id": "wri9qwDgwMco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace Transformers\n",
        "\n",
        "Next we will install the `transformers` library, built by HuggingFace. This library makes it extremely easy to use SOTA neural NLP models with PyTorch. See the HuggingFace website to browse all the publically available models: https://huggingface.co/models\n",
        "\n",
        "## HuggingFace Datasets\n",
        "HuggingFace also provides a library called `datasets` for downloading and utilizing common NLP datasets: https://huggingface.co/datasets\n",
        "\n",
        "## SentencePiece Tokenizer\n",
        "The SentencePiece tokenizer library is required for the PEGASUS model\n",
        "\n",
        "## Model Summary\n",
        "TorchInfo is a nice little library to provide a summary of model sizes and layers. We install it below to visualize the size of our models."
      ],
      "metadata": {
        "id": "EMz34AztC1lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece torchinfo"
      ],
      "metadata": {
        "id": "1YvJqRvi4tHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import datasets\n",
        "from torchinfo import summary\n",
        "from textwrap import wrap"
      ],
      "metadata": {
        "id": "9jx9xw1y4hIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)\n",
        "print('CUDA Enabled: ', torch.cuda.is_available())\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  print(f'  {device} - ' + torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print(f'  {device}')"
      ],
      "metadata": {
        "id": "RZ3Pa-paWI2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above cell should include a torch library with \"+cu...\" to denote PyTorch is installed with CUDA capabilities. CUDA should be enabled with at least one device. Typically a Tesla K80 is the GPU I get on Google Colab, but others may be assigned as resources are made available. If you are unable to reserve a GPU instance then the device will be \"cpu\" and the code will run much slower, but still work."
      ],
      "metadata": {
        "id": "ZJDQtKsTZhOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Multi-Document Summarization Model\n",
        "\n",
        "We will download the PRIMERA model and unzip as follows:"
      ],
      "metadata": {
        "id": "QX06_-otw48Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/primer_summ/PRIMER_multixscience.tar.gz"
      ],
      "metadata": {
        "id": "7oAUgatvw0dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf PRIMER_multixscience.tar.gz"
      ],
      "metadata": {
        "id": "E3mkp9AZxI4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then move the model to the `cuda:0` device (our GPU) and turn on eval mode to avoid dropout randomness.\n",
        "\n",
        "Finally, we print a summary of our model."
      ],
      "metadata": {
        "id": "OI-_5wOvcXm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "from transformers import AutoTokenizer\n",
        "from longformer import LongformerEncoderDecoderForConditionalGeneration\n",
        "from longformer import LongformerEncoderDecoderConfig\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('./PRIMER_multixscience')\n",
        "config = LongformerEncoderDecoderConfig.from_pretrained('./PRIMER_multixscience')\n",
        "model = LongformerEncoderDecoderForConditionalGeneration.from_pretrained(\n",
        "    './PRIMER_multixscience', config=config)\n",
        "\n",
        "# move model to GPU device\n",
        "model.to(device)\n",
        "# turn on EVAL mode so drop-out layers do not randomize outputs\n",
        "model.eval()\n",
        "# create model summary\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "SZsV1DWOchJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization Dataset\n",
        "We will examine the Multi-XScience Dataset. https://github.com/yaolu/Multi-XScience\n",
        "\n",
        "Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles. Multi-XScience introduces a challenging multi-document summarization task: writing therelated-work section of a paper based on itsabstract and the articles it references."
      ],
      "metadata": {
        "id": "DmQTA2_7klsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "#@title Dataset\n",
        "\n",
        "dataset = 'multi_x_science_sum' #@param [\"multi_x_science_sum\", \"multi_news\"]\n",
        "data = load_dataset(dataset)\n",
        "ds = data['validation']\n",
        "data_size = len(ds)\n",
        "print(ds)\n",
        "\n",
        "def preproces(example):\n",
        "  all_docs = [example[\"abstract\"]]\n",
        "  for d in example[\"ref_abstract\"][\"abstract\"]:\n",
        "      if len(d) > 0:\n",
        "          all_docs.append(d)\n",
        "  tgt = example[\"related_work\"]\n",
        "  # remove all @cite_d\n",
        "  tgt = re.sub(r\"\\@cite_\\d+\", \"cite\", tgt)\n",
        "  ex = {\n",
        "      \"documents\": all_docs,\n",
        "      \"summary\": tgt,\n",
        "  }\n",
        "  return ex\n",
        "\n",
        "ds = [preproces(ex) for ex in ds]"
      ],
      "metadata": {
        "id": "nTIvI2HpkmIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting the Dataset\n",
        "\n",
        "We can look at individual examples in the validation collection of SQUAD v2 to get a feeling for the types of questions and answers."
      ],
      "metadata": {
        "id": "EmA-JENXoUtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example { run: \"auto\" }\n",
        "example_index = 1069 #@param {type:\"slider\", min:0, max:5065, step:1}\n",
        "example = ds[example_index]\n",
        "print('Documents: ')\n",
        "for doc in example[\"documents\"]:\n",
        "  for line in wrap(doc, 100):\n",
        "    print(f'  {line}')\n",
        "  print()\n",
        "  \n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "TA0pl-Btlpen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specific Example\n",
        "We will use the below example to follow the prediction process of the model"
      ],
      "metadata": {
        "id": "Y-X7yze6rRNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_index = 2161\n",
        "example = ds[example_index]\n",
        "print('Documents: ')\n",
        "for doc in example[\"documents\"]:\n",
        "  for line in wrap(doc, 100):\n",
        "    print(f'  {line}')\n",
        "  print()\n",
        "  \n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "_JwW3EvPrF5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "We will tokenize the above example using the HuggingFace tokenizer:"
      ],
      "metadata": {
        "id": "sRDfru4IkeAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first we get the token for seperating documents:\n",
        "docsep_token_id = tokenizer.additional_special_tokens_ids[0]\n",
        "print(docsep_token_id)\n",
        "print(tokenizer.decode([docsep_token_id]))\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "print(pad_token_id)"
      ],
      "metadata": {
        "id": "kvzuDWf643Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_len = 4096\n",
        "input_ids = []\n",
        "for doc in example['documents']:\n",
        "  input_ids.extend(\n",
        "      tokenizer.encode(\n",
        "          doc,\n",
        "          truncation=True,\n",
        "          max_length=(max_input_len) // len(example['documents']),\n",
        "      )[1:-1]\n",
        "  )\n",
        "  input_ids.append(docsep_token_id)\n",
        "\n",
        "input_ids = (\n",
        "    [tokenizer.bos_token_id]\n",
        "    + input_ids\n",
        "    + [tokenizer.eos_token_id]\n",
        ")\n",
        "\n",
        "input_ids = torch.tensor([input_ids]).to(device)\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "94_NK73b5IDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# these are the token ids of the input. We can convert back to text like so:\n",
        "input_tokens = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "for line in wrap(str(input_tokens), 100):\n",
        "  print(line)\n",
        "\n",
        "# Notice that we have added a <s> token to the start,\n",
        "#  </s> token to denote the end of the sequence,\n",
        "# and the <doc-sep> token between sequences"
      ],
      "metadata": {
        "id": "CrrPIqMjqMke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Model\n",
        "\n",
        "Next we will run the model on the above example"
      ],
      "metadata": {
        "id": "P4GXlLhfbj7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from longformer.sliding_chunks import pad_to_window_size"
      ],
      "metadata": {
        "id": "daiphuSn9nDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the outputs will contain decoded token ids\n",
        "# based on the estimated most likely summary sequence\n",
        "# using greedy decoding\n",
        "attention_mask = torch.ones(\n",
        "    input_ids.shape, dtype=torch.long, device=input_ids.device\n",
        ")\n",
        "attention_mask[input_ids == pad_token_id] = 0\n",
        "# global attention on one token for all model params to be used,\n",
        "# which is important for gradient checkpointing to work\n",
        "attention_mask[:, 0] = 2\n",
        "attention_mask[input_ids == docsep_token_id] = 2\n",
        "# attention_mode == \"sliding_chunks\":\n",
        "half_padding_mod = model.config.attention_window[0]\n",
        "\n",
        "input_ids, attention_mask = pad_to_window_size(\n",
        "    # ideally, should be moved inside the LongformerModel\n",
        "    input_ids,\n",
        "    attention_mask,\n",
        "    half_padding_mod,\n",
        "    pad_token_id,\n",
        ")\n",
        "summary_ids = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    use_cache=True,\n",
        "    max_length=1024,\n",
        "    min_length=0,\n",
        "    num_beams=1,\n",
        "    length_penalty=1.0,\n",
        "    no_repeat_ngram_size=3\n",
        ")[0, 1:]\n",
        "print(summary_ids)"
      ],
      "metadata": {
        "id": "3SH5GFb7avK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can then transform these tokens to a normal string:\n",
        "summary = tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')\n",
        "print('Generated Summary:')\n",
        "for line in wrap(summary, 100):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "KUtPSG6Pq9p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling Summaries\n",
        "We will first define a `run_model` function to do all of the above for an example."
      ],
      "metadata": {
        "id": "kqpw8AwMr0AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run this cell when you swap models\n",
        "def run_model(example, **generate_args):\n",
        "  # we will tokenize a single example document,\n",
        "  # and we will move these tensors to the GPU device:\n",
        "  max_input_len = 4096\n",
        "  input_ids = []\n",
        "  for doc in example['documents']:\n",
        "    input_ids.extend(\n",
        "        tokenizer.encode(\n",
        "            doc,\n",
        "            truncation=True,\n",
        "            max_length=(max_input_len) // len(example['documents']),\n",
        "        )[1:-1]\n",
        "    )\n",
        "    input_ids.append(docsep_token_id)\n",
        "\n",
        "  input_ids = (\n",
        "      [tokenizer.bos_token_id]\n",
        "      + input_ids\n",
        "      + [tokenizer.eos_token_id]\n",
        "  )\n",
        "\n",
        "  input_ids = torch.tensor([input_ids]).to(device)\n",
        "\n",
        "  attention_mask = torch.ones(\n",
        "    input_ids.shape, dtype=torch.long, device=input_ids.device\n",
        "  )\n",
        "  attention_mask[input_ids == pad_token_id] = 0\n",
        "  # global attention on one token for all model params to be used,\n",
        "  # which is important for gradient checkpointing to work\n",
        "  attention_mask[:, 0] = 2\n",
        "  attention_mask[input_ids == docsep_token_id] = 2\n",
        "  # attention_mode == \"sliding_chunks\":\n",
        "  half_padding_mod = model.config.attention_window[0]\n",
        "\n",
        "  input_ids, attention_mask = pad_to_window_size(\n",
        "      # ideally, should be moved inside the LongformerModel\n",
        "      input_ids,\n",
        "      attention_mask,\n",
        "      half_padding_mod,\n",
        "      pad_token_id,\n",
        "  )\n",
        "  # the outputs will contain decoded token ids\n",
        "  # based on the estimated most likely summary sequence\n",
        "  # using various decoding options\n",
        "  multi_summary_ids = model.generate(\n",
        "      input_ids=input_ids,   \n",
        "      attention_mask=attention_mask,\n",
        "      use_cache=True,\n",
        "      max_length=1024,\n",
        "      min_length=0,\n",
        "      length_penalty=1.0,\n",
        "      no_repeat_ngram_size=3,\n",
        "      **generate_args\n",
        "  )[:, 1:]\n",
        "  # converts token ids back to strings for multiple summaries\n",
        "  summaries = tokenizer.batch_decode(\n",
        "      multi_summary_ids, \n",
        "      skip_special_tokens=True\n",
        "  )\n",
        "  return summaries\n"
      ],
      "metadata": {
        "id": "07W9NtfvsCP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Strategies\n",
        "There are various ways to produce samples from a sequence generating model. \n",
        "Above we utilized Greedy search, which picks the maximum probability token at\n",
        "every opportunity. This can miss out on other tokens which may have \n",
        "a lower conditional probability, but produce a higher joint sentence probability\n",
        "after futher token generation.\n",
        "The following article summarizes many popular generating strategies: https://huggingface.co/blog/how-to-generate"
      ],
      "metadata": {
        "id": "pJE5WYbf8jCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Search\n",
        "![](https://huggingface.co/blog/assets/02_how-to-generate/greedy_search.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "-iof4jQt9H7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = run_model(example)[0]\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')\n",
        "  \n",
        "print('Greedy Summary:')\n",
        "for line in wrap(summary, 100):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "e0-T4G9y9UK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search\n",
        "![](https://huggingface.co/blog/assets/02_how-to-generate/beam_search.png)\n"
      ],
      "metadata": {
        "id": "dwtfdKEo9Z0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    num_beams=10, \n",
        "    num_return_sequences=5, \n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Beam Summaries:')\n",
        "for beam, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Beam #{beam} Summary:')\n",
        "  for line in wrap(summary, 100):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "AZ42XEGb-dL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problems with Beam Search\n",
        "Beam search and all deterministic generating approaches are rarely suprising.\n",
        "This leads to an almost robotic sounding result, where only high-probability English words are selected. \n",
        "\n",
        "![](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n",
        "\n",
        "In reality, language is often suprising, with unlikely words showing up all the time! Therefore, we want to consider approaches which randomly sample from the conditional distribution produced by our model."
      ],
      "metadata": {
        "id": "G2hpG4JWAcO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling\n",
        "Now there are multiple approaches to random sampling from the $P(w_t|w_{1:t-1})$ conditional distribution. The first approach is just to directly sample:"
      ],
      "metadata": {
        "id": "Z-PPtxkcBG9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Sampled Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 100):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "FzPhhRPt_M7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature\n",
        "\n",
        "We can modify $P(w_t|w_{1:t-1})$ to be more or less \"suprising\", by making the distribution sharper or more flat with the `temperature` parameter. A lower temperature ($t<1.0$) leads to a sharper distribution, which will have a higher probability of sampling from high probability tokens.\n"
      ],
      "metadata": {
        "id": "EjwnDruNBgPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_k=0,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Low Temperature Sampled Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 100):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "ivyMZJDh_gyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A higher temperature ($t>1.0$) leads to a flatter distribution, which will have a higher probability of sampling from low probability tokens."
      ],
      "metadata": {
        "id": "SnLxtDFuCNju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_k=0,\n",
        "    temperature=1.3\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('High Temperature Sampled Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 100):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "ugymlabo_qBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-K Sampling\n",
        "Top-K sampling restricts $P(w_t|w_{1:t-1})$ to only allow sampling from the top-k probability tokens. In effect, this rebalances $P(w_t|w_{1:t-1})$ to remove all probability mass from non top-k tokens to be redistributed to top-k tokens, such that only top-k tokens get sampled. This approach avoids sampling extremely low probability tokens, and thus potentially ruining the sequence."
      ],
      "metadata": {
        "id": "Z30xo7WLCRAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_k=50,\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Top-K Sampling Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 100):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "7CS0RpVG_vP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-P Sampling\n",
        "Top-P sampling restricts $P(w_t|w_{1:t-1})$ to only allow sampling from the tokens which have a sum total probability mass greater than p. In other words, the probabilities $P(w_t|w_{1:t-1})$ are sorted, from largest to smallest, and only tokens from the first top-p probability mass are available to be sampled from. The probability mass is then redistributed among these top-p tokens."
      ],
      "metadata": {
        "id": "sIEbvfI8CyMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_p=0.90, \n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Top-P Sampling Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 100):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "95Kg-EKi_6gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-P and Top-K Sampling\n",
        "We can also perform both Top-P and Top-K sampling together, which provides multiple constraints on which tokens we can sample from $P(w_t|w_{1:t-1})$."
      ],
      "metadata": {
        "id": "gOHs5QFBDsD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = run_model(\n",
        "    example,\n",
        "    do_sample=True, \n",
        "    num_return_sequences=5,\n",
        "    top_p=0.90, \n",
        "    top_k=50, \n",
        ")\n",
        "\n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')\n",
        "\n",
        "print('Top-P AND Top-K Sampling Summaries:')\n",
        "for sample, summary in enumerate(summaries, start=1):\n",
        "  print(f'  Sample #{sample} Summary:')\n",
        "  for line in wrap(summary, 100):\n",
        "    print(f'    {line}')"
      ],
      "metadata": {
        "id": "tgGREitAAKyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examine Summaries\n",
        "Now we will perform the above process for a few examples. We will first define a `generate` function to do all of the above for an example."
      ],
      "metadata": {
        "id": "QumL1aaZswfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(example, strategy):\n",
        "  if strategy == 'greedy':\n",
        "    summary = run_model(example)[0]\n",
        "  elif strategy == 'beam':\n",
        "    summary = run_model(\n",
        "        example,\n",
        "        num_beams=10, \n",
        "        num_return_sequences=1, \n",
        "        early_stopping=True\n",
        "    )[0]\n",
        "  elif strategy == 'sample':\n",
        "    summary = run_model(\n",
        "        example,\n",
        "        do_sample=True, \n",
        "        num_return_sequences=1,\n",
        "        top_k=0, \n",
        "    )[0]\n",
        "  elif strategy == 'top-k':\n",
        "    summary = run_model(\n",
        "        example,\n",
        "        do_sample=True, \n",
        "        num_return_sequences=1,\n",
        "        top_k=50, \n",
        "    )[0]\n",
        "  elif strategy == 'top-p':\n",
        "    summary = run_model(\n",
        "        example,\n",
        "        do_sample=True, \n",
        "        num_return_sequences=1,\n",
        "        top_p=0.90, \n",
        "        top_k=0, \n",
        "    )[0]\n",
        "  elif strategy == 'top-p-k':\n",
        "    summary = run_model(\n",
        "        example,\n",
        "        do_sample=True, \n",
        "        num_return_sequences=1,\n",
        "        top_p=0.90, \n",
        "        top_k=50, \n",
        "    )[0]\n",
        "  else:\n",
        "    raise ValueError(f'Unknown generator strategy: {strategy}')\n",
        "  return summary"
      ],
      "metadata": {
        "id": "cU95bOoEIlyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "Change the example index and view the model's predictions below for 10 different examples. For each example, compare the results for each strategy. Manually judge whether each strategy for each of the 10 examples is correct, for a total of 60 judgements. \n",
        "\n",
        "Discuss how accurately the model summarized the documents and whether they lined up with the annotated summaries of the examples. Report the results in your report. "
      ],
      "metadata": {
        "id": "_9T_CyZuKHlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example { run: \"auto\" }\n",
        "example_index = 0 #@param {type:\"slider\", min:0, max:11331, step:1}\n",
        "strategy = 'top-p-k' #@param [\"greedy\", \"beam\", \"sample\", \"top-k\", \"top-p\", \"top-p-k\"]\n",
        "\n",
        "example = ds[example_index]\n",
        "print('Documents: ')\n",
        "for doc in example[\"documents\"]:\n",
        "  for line in wrap(doc, 100):\n",
        "    print(f'  {line}')\n",
        "  print()\n",
        "  \n",
        "print('Annotated Summary: ')\n",
        "for line in wrap(example['summary'], 100):\n",
        "  print(f'  {line}')\n",
        "\n",
        "summary = generate(example, strategy)\n",
        "\n",
        "print(f'Generated Summary: ')\n",
        "for line in wrap(summary, 100):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "Ix5Olgx9q-x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report Format\n",
        "\n",
        "You should have the following in your report:\n",
        "\n",
        "| Strategy      | Accuracy |\n",
        "| ----------- | ----------- |\n",
        "| greedy      | ...       |\n",
        "| beam      | ...       |\n",
        "| sample      | ...       |\n",
        "| top-k      | ...       |\n",
        "| top-p      | ...       |\n",
        "| top-p-k      | ...       |\n",
        "\n",
        "\n",
        "Calculate the accuracy of each summary strategy by adding up the number of correct examples (by your own judgement) and dividing by 10 (the total number of examples you should evaluate). \n",
        "\n",
        "Also include an example prediction that has a judged answer and compare it to the predictions by each strategy. Try to find an example where the strategies differ."
      ],
      "metadata": {
        "id": "A0f3TGGVuhrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mGUQP0Awsdg8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}