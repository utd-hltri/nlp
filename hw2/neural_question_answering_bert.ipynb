{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural_question_answering_bert.ipynb",
      "version": "0.3.2",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
        "cell_type": "markdown",
        "metadata": {
          "id": "view-in-github",
          "colab_type": "text"
        },
        "source": [
          "<a href=\"https://colab.research.google.com/github/utd-hltri/nlp/blob/main/hw2/neural_question_answering_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
        ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx1OQd2_WHok"
      },
      "outputs": [],
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2022 Maxwell Weinzierl\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering with Bidirectional Encoder Representations from Transformers (BERT)\n",
        "\n",
        "This notebook utilizes BERT, a State-Of-The-Art (SOTA) neural transformer model pre-trained on masked language modeling and next sentence prediction. \n",
        "The paper which introduces BERT can be found here:\n",
        "https://arxiv.org/pdf/1810.04805.pdf\n",
        "\n",
        "We also use RoBERTa: A Robustly Optimized BERT Pretraining Approach which is a better-optimized BERT model: https://arxiv.org/pdf/1907.11692.pdf\n",
        "\n",
        "Finally, we use a distilled RoBERTa model, which is a technique which attempts to take a much larger model and distill its knowledge into a smaller model: https://arxiv.org/pdf/1503.02531.pdf"
      ],
      "metadata": {
        "id": "IpFmigG0XWC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages and Libraries\n",
        "We will utilize the deep learning library PyTorch this time as opposed to TensorFlow. PyTorch (https://pytorch.org/) has become the most popular deep learning library for research to-date: http://horace.io/pytorch-vs-tensorflow/\n",
        "\n",
        "![](https://www.assemblyai.com/blog/content/images/2021/12/Fraction-of-Papers-Using-PyTorch-vs.-TensorFlow.png)"
      ],
      "metadata": {
        "id": "WouzhYvvYBFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print('CUDA Enabled: ', torch.cuda.is_available())\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  print(f'  {device} - ' + torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  print(f'  {device}')"
      ],
      "metadata": {
        "id": "RZ3Pa-paWI2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above cell should include a torch library with \"+cu...\" to denote PyTorch is installed with CUDA capabilities. CUDA should be enabled with at least one device. Typically a Tesla K80 is the GPU I get on Google Colab, but others may be assigned as resources are made available. If you are unable to reserve a GPU instance then the device will be \"cpu\" and the code will run much slower, but still work."
      ],
      "metadata": {
        "id": "ZJDQtKsTZhOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace Transformers\n",
        "\n",
        "Next we will install the `transformers` library, built by HuggingFace. This library makes it extremely easy to use SOTA neural NLP models with PyTorch. See the HuggingFace website to browse all the publically available models: https://huggingface.co/models"
      ],
      "metadata": {
        "id": "hNaMvseCZ9A5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "TQ9hU2mJWsNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "qIYbdAHIWvcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HuggingFace Datasets\n",
        "HuggingFace also provides a library called `datasets` for downloading and utilizing common NLP datasets: https://huggingface.co/datasets"
      ],
      "metadata": {
        "id": "nkH-Mxn4abAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "blP2TBkpW_V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "print(datasets.__version__)"
      ],
      "metadata": {
        "id": "jTvfEf1Lam1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Summary\n",
        "\n",
        "TorchInfo is a nice little library to provide a summary of model sizes and layers. We install it below to visualize the size of our models."
      ],
      "metadata": {
        "id": "ZJKkjgmehMba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "pzyvboUFhSiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary"
      ],
      "metadata": {
        "id": "PMBaKtn6hOfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Question Answering Models\n",
        "\n",
        "Below we load our neural QA model. We load the model and the tokenizer from the `model_name` from HuggingFace. The library will automatically download all required model weights, config files, and tokenizers.\n",
        "\n",
        "We then move the model to the `cuda:0` device (our GPU) and turn on eval mode to avoid dropout randomness.\n",
        "\n",
        "Finally, we print a summary of our model."
      ],
      "metadata": {
        "id": "OI-_5wOvcXm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "model_name = 'deepset/bert-base-cased-squad2' #@param [\"deepset/bert-base-cased-squad2\", \"deepset/roberta-base-squad2\", \"deepset/roberta-base-squad2-distilled\"]\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# move model to GPU device\n",
        "model.to(device)\n",
        "# turn on EVAL mode so drop-out layers do not randomize outputs\n",
        "model.eval()\n",
        "# create model summary\n",
        "summary(model)"
      ],
      "metadata": {
        "id": "SZsV1DWOchJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering Datasets\n",
        "The largest and most utilized Question-Answering dataset is The Stanford Question Answering Dataset (SQUAD): https://rajpurkar.github.io/SQuAD-explorer/\n",
        "\n",
        "They have released a 2.0 version of SQUAD which we will utilize below. Feel free to play around with the other QA datasets, or find your own on HuggingFace Datasets: https://huggingface.co/datasets?task_categories=task_categories:question-answering&sort=downloads"
      ],
      "metadata": {
        "id": "DmQTA2_7klsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# imported to help inspect dataset\n",
        "from textwrap import wrap\n",
        "\n",
        "#@title Dataset\n",
        "\n",
        "dataset = 'squad_v2' #@param [\"squad_v2\", \"squad\", \"adversarial_qa\"]\n",
        "data = load_dataset(dataset)\n",
        "ds = data['validation']\n",
        "data_size = len(ds)\n",
        "print(ds)"
      ],
      "metadata": {
        "id": "nTIvI2HpkmIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting the Dataset\n",
        "\n",
        "We can look at individual examples in the validation collection of SQUAD v2 to get a feeling for the types of questions and answers."
      ],
      "metadata": {
        "id": "EmA-JENXoUtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example { run: \"auto\" }\n",
        "example_index = 0 #@param {type:\"slider\", min:0, max:11872, step:1}\n",
        "example = ds[example_index]\n",
        "print('Question: ')\n",
        "for line in wrap(example['question'], 50):\n",
        "  print(f'  {line}')\n",
        "print('Context: ')\n",
        "for line in wrap(example['context'], 50):\n",
        "  print(f'  {line}')\n",
        "answer = 'No Answer Provided' if len(example['answers']['text']) == 0 else example['answers']['text'][0]\n",
        "print(f'Answer: ')\n",
        "for line in wrap(answer, 50):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "TA0pl-Btlpen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Specific Example\n",
        "We will use the below example to follow the prediction process of the model"
      ],
      "metadata": {
        "id": "Y-X7yze6rRNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_index = 339\n",
        "example = ds[example_index]\n",
        "print('Question: ')\n",
        "for line in wrap(example['question'], 50):\n",
        "  print(f'  {line}')\n",
        "print('Context: ')\n",
        "for line in wrap(example['context'], 50):\n",
        "  print(f'  {line}')\n",
        "answer = 'No Answer Provided' if len(example['answers']['text']) == 0 else example['answers']['text'][0]\n",
        "print(f'Answer: ')\n",
        "for line in wrap(answer, 50):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "_JwW3EvPrF5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "We will tokenize the above example using the HuggingFace tokenizer:"
      ],
      "metadata": {
        "id": "sRDfru4IkeAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will tokenize a single example question and context,\n",
        "# and we will move these tensors to the GPU device:\n",
        "inputs = tokenizer(example['question'], example['context'], return_tensors=\"pt\").to(device)\n",
        "\n",
        "print('Inputs to model: ')\n",
        "print(f'  {inputs.keys()}')"
      ],
      "metadata": {
        "id": "B_j4dH1tfiGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the inputs to the model will contain a few tensors, but the most\n",
        "# important tensor is the \"input_ids\":\n",
        "input_ids = inputs['input_ids'][0]\n",
        "print(input_ids)"
      ],
      "metadata": {
        "id": "BhBBZvvbqIRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# these are the token ids of the input. We can convert back to text tokens like so:\n",
        "input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "for line in wrap(str(input_tokens), 50):\n",
        "  print(line)\n",
        "\n",
        "# Notice that we have added a [CLS] token to denote the start of the sequence,\n",
        "# [SEP] tokens between the question and context and at the end of the sequence,\n",
        "# and the Word-Piece tokenizer has split some words up into pieces, such as \n",
        "# 'Turin', '##g' from Turing and \n",
        "# 'de', '##via', '##tes' from deviates"
      ],
      "metadata": {
        "id": "CrrPIqMjqMke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Model\n",
        "\n",
        "Next we will run the model on the above example"
      ],
      "metadata": {
        "id": "P4GXlLhfbj7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# the outputs will contain logits (unnormalized probabilities) for the start and the end of the answer sequence.\n",
        "outputs = model(**inputs)\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "id": "3SH5GFb7avK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we select the most likely start of the answer by taking the maximum start logit (probability)\n",
        "answer_start = torch.argmax(outputs['start_logits'])\n",
        "\n",
        "# we also select the most likely end of the answer by taking the maximum end logit (probability)\n",
        "answer_end = torch.argmax(outputs['end_logits'])\n",
        "\n",
        "print(f'Answer Token Span: {answer_start} to {answer_end}')"
      ],
      "metadata": {
        "id": "rRiFg2qoffqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we can now retrieve the most likely answer to the question from the input:\n",
        "answer_ids = input_ids[answer_start:answer_end+1]\n",
        "print(answer_ids)\n"
      ],
      "metadata": {
        "id": "-NXspT5Tq8mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we convert these token ids back to tokens: \n",
        "answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
        "print(answer_tokens)"
      ],
      "metadata": {
        "id": "UNBQVZAyrtQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can then transform these tokens to a normal string:\n",
        "answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "print(f'Answer: {answer}')"
      ],
      "metadata": {
        "id": "KUtPSG6Pq9p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examine QA Predictions\n",
        "Now we will perform the above process for a few examples. We will first define a `run_model` function to do all of the above for an example"
      ],
      "metadata": {
        "id": "kqpw8AwMr0AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run this cell when you swap models\n",
        "def run_model(example):\n",
        "  # we will tokenize a single example question and context,\n",
        "  # and we will move these tensors to the GPU device:\n",
        "  inputs = tokenizer(example['question'], example['context'], return_tensors=\"pt\").to(device)\n",
        "  # the inputs to the model will contain a few tensors, but the most\n",
        "  # important tensor is the \"input_ids\":\n",
        "  input_ids = inputs['input_ids'][0]\n",
        "  # these are the token ids of the input. We can convert back to text tokens like so:\n",
        "  input_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "  # the outputs will contain logits (unnormalized probabilities) for the start and the end of the answer sequence.\n",
        "  outputs = model(**inputs)\n",
        "  # we select the most likely start of the answer by taking the maximum start logit (probability)\n",
        "  answer_start = torch.argmax(outputs['start_logits'])\n",
        "\n",
        "  # we also select the most likely end of the answer by taking the maximum end logit (probability)\n",
        "  answer_end = torch.argmax(outputs['end_logits'])\n",
        "\n",
        "  # we can now retrieve the most likely answer to the question from the input:\n",
        "  answer_ids = input_ids[answer_start:answer_end+1]\n",
        "\n",
        "  # we convert these token ids back to tokens: \n",
        "  answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids)\n",
        "  # we can then transform these tokens to a normal string:\n",
        "  answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "  return answer.strip()\n"
      ],
      "metadata": {
        "id": "07W9NtfvsCP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "\n",
        "Change the example index and view the model's predictions below for 10 different examples and report the results in your report. Discuss how accurately the model predicted answers and whether they lined up with the judged answers of the examples. \n",
        "\n",
        "IMPORTANT: keep track of the example indices you evaluate, you will need them when evaluating new models!"
      ],
      "metadata": {
        "id": "QumL1aaZswfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Example { run: \"auto\" }\n",
        "example_index = 5465 #@param {type:\"slider\", min:0, max:11872, step:1}\n",
        "example = ds[example_index]\n",
        "print('Question: ')\n",
        "for line in wrap(example['question'], 50):\n",
        "  print(f'  {line}')\n",
        "print('Context: ')\n",
        "for line in wrap(example['context'], 50):\n",
        "  print(f'  {line}')\n",
        "answer = 'No Answer Provided' if len(example['answers']['text']) == 0 else example['answers']['text'][0]\n",
        "print(f'Answer: ')\n",
        "for line in wrap(answer, 50):\n",
        "  print(f'  {line}')\n",
        "\n",
        "p_answer = run_model(example)\n",
        "\n",
        "print(f'Predicted Answer: ')\n",
        "for line in wrap(p_answer, 50):\n",
        "  print(f'  {line}')"
      ],
      "metadata": {
        "id": "Ix5Olgx9q-x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Models and Report\n",
        "\n",
        "Go back to the cell in which you loaded the neural QA model and perform the same above evaluation with the other two neural models. Compare the outputs of each model across the same example indices and report your results in your report. Make sure you re-load the run_model function cell when you change your model. You do not need to re-run the Question Answering Datasets or Specific Example section cells.\n",
        "\n",
        "You should have the following in your report:\n",
        "\n",
        "| Model      | Accuracy |\n",
        "| ----------- | ----------- |\n",
        "| bert-base-cased-squad2      | ...       |\n",
        "| roberta-base-squad2   | ...        |\n",
        "| roberta-base-squad2-distilled   | ...        |\n",
        "\n",
        "\n",
        "Calculate the accuracy of each model by adding up the number of correct examples (by your own judgement) and dividing by 10 (the total number of examples you should evaluate). If no judged answer exists and the model outputs nothing or \"&lt;s>\" then consider that correct. Otherwise use your own judgement.\n",
        "\n",
        "Also include an example prediction that has a judged answer and compare it to the predictions by each system. Try to find an example where the systems differ in their predictions."
      ],
      "metadata": {
        "id": "A0f3TGGVuhrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mGUQP0Awsdg8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}